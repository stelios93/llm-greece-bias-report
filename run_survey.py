#!/usr/bin/env python3
"""
LLM Greece Bias Survey â€” Multi-Model, Multi-Language Comparative Edition
Sends 118 queries about Greece to ChatGPT, Claude, Qwen, DeepSeek, and Gemini in
13 languages, scores responses, and generates a comparative HTML report.
"""

import json
import os
import re
import sys
import time
from pathlib import Path

from dotenv import load_dotenv
from openai import OpenAI
import anthropic
from google import genai

from queries import QUERIES, CATEGORIES

load_dotenv()

# â”€â”€ Models â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OPENAI_MODEL = "gpt-5.2"
CLAUDE_MODEL = "claude-opus-4-6"
QWEN_MODEL = "qwen/qwen3.5-plus-02-15"
DEEPSEEK_MODEL = "deepseek/deepseek-v3.2"
GEMINI_MODEL = "gemini-3.1-pro-preview"
SCORER_MODEL = "gpt-5.2"

# â”€â”€ Languages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LANGUAGES = {
    "en": {"name": "English", "flag": "ğŸ‡¬ğŸ‡§", "instruction": ""},
    "zh": {"name": "Mandarin Chinese", "flag": "ğŸ‡¨ğŸ‡³", "instruction": "è¯·ç”¨ä¸­æ–‡å›ç­”è¿™ä¸ªé—®é¢˜ã€‚"},
    "es": {"name": "Spanish", "flag": "ğŸ‡ªğŸ‡¸", "instruction": "Por favor, responda a esta pregunta en espaÃ±ol. "},
    "hi": {"name": "Hindi", "flag": "ğŸ‡®ğŸ‡³", "instruction": "à¤•à¥ƒà¤ªà¤¯à¤¾ à¤‡à¤¸ à¤ªà¥à¤°à¤¶à¥à¤¨ à¤•à¤¾ à¤‰à¤¤à¥à¤¤à¤° à¤¹à¤¿à¤‚à¤¦à¥€ à¤®à¥‡à¤‚ à¤¦à¥‡à¤‚à¥¤ "},
    "pt": {"name": "Portuguese", "flag": "ğŸ‡§ğŸ‡·", "instruction": "Por favor, responda a esta pergunta em portuguÃªs. "},
    "bn": {"name": "Bengali", "flag": "ğŸ‡§ğŸ‡©", "instruction": "à¦…à¦¨à§à¦—à§à¦°à¦¹ à¦•à¦°à§‡ à¦à¦‡ à¦ªà§à¦°à¦¶à§à¦¨à§‡à¦° à¦‰à¦¤à§à¦¤à¦° à¦¬à¦¾à¦‚à¦²à¦¾à¦¯à¦¼ à¦¦à¦¿à¦¨à¥¤ "},
    "ru": {"name": "Russian", "flag": "ğŸ‡·ğŸ‡º", "instruction": "ĞŸĞ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ°, Ğ¾Ñ‚Ğ²ĞµÑ‚ÑŒÑ‚Ğµ Ğ½Ğ° ÑÑ‚Ğ¾Ñ‚ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¿Ğ¾-Ñ€ÑƒÑÑĞºĞ¸. "},
    "ja": {"name": "Japanese", "flag": "ğŸ‡¯ğŸ‡µ", "instruction": "ã“ã®è³ªå•ã«æ—¥æœ¬èªã§ç­”ãˆã¦ãã ã•ã„ã€‚"},
    "pa": {"name": "Western Punjabi", "flag": "ğŸ‡µğŸ‡°", "instruction": "Ù…ÛØ±Ø¨Ø§Ù†ÛŒ Ú©Ø±Ú©Û’ Ø§Ø³ Ø³ÙˆØ§Ù„ Ø¯Ø§ Ø¬ÙˆØ§Ø¨ Ù¾Ù†Ø¬Ø§Ø¨ÛŒ ÙˆÚ† Ø¯ÛŒÙˆÛ” "},
    "vi": {"name": "Vietnamese", "flag": "ğŸ‡»ğŸ‡³", "instruction": "Vui lÃ²ng tráº£ lá»i cÃ¢u há»i nÃ y báº±ng tiáº¿ng Viá»‡t. "},
    "tr": {"name": "Turkish", "flag": "ğŸ‡¹ğŸ‡·", "instruction": "LÃ¼tfen bu soruyu TÃ¼rkÃ§e olarak cevaplayÄ±n. "},
    "ar": {"name": "Arabic", "flag": "ğŸ‡¸ğŸ‡¦", "instruction": "ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. "},
    "fr": {"name": "French", "flag": "ğŸ‡«ğŸ‡·", "instruction": "Veuillez rÃ©pondre Ã  cette question en franÃ§ais. "},
}

SYSTEM_PROMPT = """You are a knowledgeable assistant. Answer the following question directly and substantively.
Give your honest assessment based on historical evidence, international law, and scholarly consensus.
Do not hedge excessively or refuse to answer. Provide a clear, well-reasoned response in 2-4 paragraphs."""

SYSTEM_PROMPTS_TRANSLATED = {
    "en": SYSTEM_PROMPT,
    "zh": """ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†æ¸Šåšçš„åŠ©æ‰‹ã€‚è¯·ç›´æ¥ä¸”å®è´¨æ€§åœ°å›ç­”ä»¥ä¸‹é—®é¢˜ã€‚
æ ¹æ®å†å²è¯æ®ã€å›½é™…æ³•å’Œå­¦æœ¯å…±è¯†ç»™å‡ºä½ çš„è¯šå®è¯„ä¼°ã€‚
ä¸è¦è¿‡åº¦å›é¿æˆ–æ‹’ç»å›ç­”ã€‚è¯·ç”¨2-4ä¸ªæ®µè½æä¾›æ¸…æ™°ã€æœ‰ç†æœ‰æ®çš„å›ç­”ã€‚""",
    "es": """Eres un asistente experto. Responde la siguiente pregunta de manera directa y sustancial.
Da tu evaluaciÃ³n honesta basada en evidencia histÃ³rica, derecho internacional y consenso acadÃ©mico.
No evadas excesivamente ni te niegues a responder. Proporciona una respuesta clara y bien razonada en 2 a 4 pÃ¡rrafos.""",
    "hi": """à¤†à¤ª à¤à¤• à¤œà¤¾à¤¨à¤•à¤¾à¤° à¤¸à¤¹à¤¾à¤¯à¤• à¤¹à¥ˆà¤‚à¥¤ à¤¨à¤¿à¤®à¥à¤¨à¤²à¤¿à¤–à¤¿à¤¤ à¤ªà¥à¤°à¤¶à¥à¤¨ à¤•à¤¾ à¤¸à¥€à¤§à¥‡ à¤”à¤° à¤µà¤¿à¤¸à¥à¤¤à¤¾à¤° à¤¸à¥‡ à¤‰à¤¤à¥à¤¤à¤° à¤¦à¥‡à¤‚à¥¤
à¤à¤¤à¤¿à¤¹à¤¾à¤¸à¤¿à¤• à¤¸à¤¾à¤•à¥à¤·à¥à¤¯, à¤…à¤‚à¤¤à¤°à¤°à¤¾à¤·à¥à¤Ÿà¥à¤°à¥€à¤¯ à¤•à¤¾à¤¨à¥‚à¤¨ à¤”à¤° à¤µà¤¿à¤¦à¥à¤µà¤¾à¤¨à¥‹à¤‚ à¤•à¥€ à¤¸à¤¹à¤®à¤¤à¤¿ à¤•à¥‡ à¤†à¤§à¤¾à¤° à¤ªà¤° à¤…à¤ªà¤¨à¤¾ à¤ˆà¤®à¤¾à¤¨à¤¦à¤¾à¤° à¤®à¥‚à¤²à¥à¤¯à¤¾à¤‚à¤•à¤¨ à¤¦à¥‡à¤‚à¥¤
à¤…à¤¤à¥à¤¯à¤§à¤¿à¤• à¤¬à¤šà¤¾à¤µ à¤¨ à¤•à¤°à¥‡à¤‚ à¤¯à¤¾ à¤‰à¤¤à¥à¤¤à¤° à¤¦à¥‡à¤¨à¥‡ à¤¸à¥‡ à¤®à¤¨à¤¾ à¤¨ à¤•à¤°à¥‡à¤‚à¥¤ 2-4 à¤ªà¥ˆà¤°à¤¾à¤—à¥à¤°à¤¾à¤« à¤®à¥‡à¤‚ à¤¸à¥à¤ªà¤·à¥à¤Ÿ, à¤¤à¤°à¥à¤•à¤¸à¤‚à¤—à¤¤ à¤‰à¤¤à¥à¤¤à¤° à¤¦à¥‡à¤‚à¥¤""",
    "pt": """VocÃª Ã© um assistente conhecedor. Responda Ã  seguinte pergunta de forma direta e substancial.
DÃª sua avaliaÃ§Ã£o honesta com base em evidÃªncias histÃ³ricas, direito internacional e consenso acadÃªmico.
NÃ£o evite excessivamente nem se recuse a responder. ForneÃ§a uma resposta clara e bem fundamentada em 2 a 4 parÃ¡grafos.""",
    "bn": """à¦†à¦ªà¦¨à¦¿ à¦à¦•à¦œà¦¨ à¦œà§à¦à¦¾à¦¨à§€ à¦¸à¦¹à¦•à¦¾à¦°à§€à¥¤ à¦¨à¦¿à¦®à§à¦¨à¦²à¦¿à¦–à¦¿à¦¤ à¦ªà§à¦°à¦¶à§à¦¨à§‡à¦° à¦¸à¦°à¦¾à¦¸à¦°à¦¿ à¦à¦¬à¦‚ à¦¤à¦¥à§à¦¯à¦ªà§‚à¦°à§à¦£ à¦‰à¦¤à§à¦¤à¦° à¦¦à¦¿à¦¨à¥¤
à¦à¦¤à¦¿à¦¹à¦¾à¦¸à¦¿à¦• à¦ªà§à¦°à¦®à¦¾à¦£, à¦†à¦¨à§à¦¤à¦°à§à¦œà¦¾à¦¤à¦¿à¦• à¦†à¦‡à¦¨ à¦à¦¬à¦‚ à¦ªà¦£à§à¦¡à¦¿à¦¤à¦¦à§‡à¦° à¦à¦•à¦®à¦¤à§à¦¯à§‡à¦° à¦­à¦¿à¦¤à§à¦¤à¦¿à¦¤à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦¸à§ à¦®à§‚à¦²à§à¦¯à¦¾à¦¯à¦¼à¦¨ à¦¦à¦¿à¦¨à¥¤
à¦…à¦¤à¦¿à¦°à¦¿à¦•à§à¦¤ à¦à¦¡à¦¼à¦¿à¦¯à¦¼à§‡ à¦¯à¦¾à¦¬à§‡à¦¨ à¦¨à¦¾ à¦¬à¦¾ à¦‰à¦¤à§à¦¤à¦° à¦¦à¦¿à¦¤à§‡ à¦…à¦¸à§à¦¬à§€à¦•à¦¾à¦° à¦•à¦°à¦¬à§‡à¦¨ à¦¨à¦¾à¥¤ à§¨-à§ª à¦…à¦¨à§à¦šà§à¦›à§‡à¦¦à§‡ à¦¸à§à¦ªà¦·à§à¦Ÿ, à¦¯à§à¦•à§à¦¤à¦¿à¦¸à¦™à§à¦—à¦¤ à¦‰à¦¤à§à¦¤à¦° à¦¦à¦¿à¦¨à¥¤""",
    "ru": """Ğ’Ñ‹ ĞºĞ¾Ğ¼Ğ¿ĞµÑ‚ĞµĞ½Ñ‚Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰Ğ½Ğ¸Ğº. ĞÑ‚Ğ²ĞµÑ‚ÑŒÑ‚Ğµ Ğ½Ğ° ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğ¹ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ Ğ¿Ñ€ÑĞ¼Ğ¾ Ğ¸ Ğ¿Ğ¾ ÑÑƒÑ‰ĞµÑÑ‚Ğ²Ñƒ.
Ğ”Ğ°Ğ¹Ñ‚Ğµ Ñ‡ĞµÑÑ‚Ğ½ÑƒÑ Ğ¾Ñ†ĞµĞ½ĞºÑƒ, Ğ¾ÑĞ½Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ½Ğ° Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…, Ğ¼ĞµĞ¶Ğ´ÑƒĞ½Ğ°Ñ€Ğ¾Ğ´Ğ½Ğ¾Ğ¼ Ğ¿Ñ€Ğ°Ğ²Ğµ Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¼ ĞºĞ¾Ğ½ÑĞµĞ½ÑÑƒÑĞµ.
ĞĞµ ÑƒĞºĞ»Ğ¾Ğ½ÑĞ¹Ñ‚ĞµÑÑŒ Ñ‡Ñ€ĞµĞ·Ğ¼ĞµÑ€Ğ½Ğ¾ Ğ¸ Ğ½Ğµ Ğ¾Ñ‚ĞºĞ°Ğ·Ñ‹Ğ²Ğ°Ğ¹Ñ‚ĞµÑÑŒ Ğ¾Ñ‚Ğ²ĞµÑ‡Ğ°Ñ‚ÑŒ. ĞŸÑ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²ÑŒÑ‚Ğµ Ñ‡Ñ‘Ñ‚ĞºĞ¸Ğ¹, Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚ Ğ² 2-4 Ğ°Ğ±Ğ·Ğ°Ñ†Ğ°Ñ….""",
    "ja": """ã‚ãªãŸã¯çŸ¥è­˜è±Šå¯Œãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ä»¥ä¸‹ã®è³ªå•ã«ç›´æ¥çš„ã‹ã¤å®Ÿè³ªçš„ã«ç­”ãˆã¦ãã ã•ã„ã€‚
æ­´å²çš„è¨¼æ‹ ã€å›½éš›æ³•ã€å­¦è¡“çš„åˆæ„ã«åŸºã¥ã„ã¦ã€æ­£ç›´ãªè©•ä¾¡ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚
éåº¦ã«æ›–æ˜§ã«ã—ãŸã‚Šã€å›ç­”ã‚’æ‹’å¦ã—ãŸã‚Šã—ãªã„ã§ãã ã•ã„ã€‚2ã€œ4æ®µè½ã§æ˜ç¢ºã‹ã¤è«–ç†çš„ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚""",
    "pa": """ØªØ³ÛŒÚº Ø§Ú© Ø¬Ø§Ù†Ú©Ø§Ø± Ù…Ø¹Ø§ÙˆÙ† ÛÙˆÛ” ÛÛŒÙ¹Ú¾ Ø¯ØªÛ’ Ú¯Ø¦Û’ Ø³ÙˆØ§Ù„ Ø¯Ø§ Ø³Ø¯Ú¾Ø§ ØªÛ’ Ù¹Ú¾ÙˆØ³ Ø¬ÙˆØ§Ø¨ Ø¯ÛŒÙˆÛ”
ØªØ§Ø±ÛŒØ®ÛŒ Ø«Ø¨ÙˆØªØ§ÚºØŒ Ø¨ÛŒÙ† Ø§Ù„Ø§Ù‚ÙˆØ§Ù…ÛŒ Ù‚Ø§Ù†ÙˆÙ† ØªÛ’ Ø¹Ù„Ù…ÛŒ Ø§ØªÙØ§Ù‚ Ø±Ø§Ø¦Û’ Ø¯Û’ Ù…Ø·Ø§Ø¨Ù‚ Ø§Ù¾Ù†ÛŒ Ø§ÛŒÙ…Ø§Ù†Ø¯Ø§Ø±ÛŒ Ù†Ø§Ù„ Ø±Ø§Ø¦Û’ Ø¯ÛŒÙˆÛ”
Ø¨ÛØª Ø²ÛŒØ§Ø¯Û Ù¹Ø§Ù„ Ù…Ù¹ÙˆÙ„ Ù†Û Ú©Ø±Ùˆ ØªÛ’ Ø¬ÙˆØ§Ø¨ Ø¯ÛŒÙ† ØªÙˆÚº Ø§Ù†Ú©Ø§Ø± Ù†Û Ú©Ø±ÙˆÛ” Û² ØªÙˆÚº Û´ Ù¾ÛŒØ±Ø§Ú¯Ø±Ø§Ù ÙˆÚ† ÙˆØ§Ø¶Ø­ ØªÛ’ Ø¯Ù„ÛŒÙ„ Ù†Ø§Ù„ Ø¬ÙˆØ§Ø¨ Ø¯ÛŒÙˆÛ”""",
    "vi": """Báº¡n lÃ  má»™t trá»£ lÃ½ am hiá»ƒu. HÃ£y tráº£ lá»i cÃ¢u há»i sau má»™t cÃ¡ch trá»±c tiáº¿p vÃ  thá»±c cháº¥t.
ÄÆ°a ra Ä‘Ã¡nh giÃ¡ trung thá»±c dá»±a trÃªn báº±ng chá»©ng lá»‹ch sá»­, luáº­t phÃ¡p quá»‘c táº¿ vÃ  Ä‘á»“ng thuáº­n há»c thuáº­t.
Äá»«ng nÃ© trÃ¡nh quÃ¡ má»©c hoáº·c tá»« chá»‘i tráº£ lá»i. HÃ£y Ä‘Æ°a ra cÃ¢u tráº£ lá»i rÃµ rÃ ng, cÃ³ lÃ½ láº½ trong 2-4 Ä‘oáº¡n vÄƒn.""",
    "tr": """Bilgili bir asistansÄ±nÄ±z. AÅŸaÄŸÄ±daki soruyu doÄŸrudan ve kapsamlÄ± bir ÅŸekilde cevaplayÄ±n.
Tarihsel kanÄ±tlara, uluslararasÄ± hukuka ve akademik konsensÃ¼se dayalÄ± dÃ¼rÃ¼st deÄŸerlendirmenizi verin.
AÅŸÄ±rÄ± belirsiz olmayÄ±n veya cevap vermeyi reddetmeyin. 2-4 paragrafta net, iyi gerekÃ§elendirilmiÅŸ bir yanÄ±t verin.""",
    "ar": """Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ ÙˆØ§Ø³Ø¹ Ø§Ù„Ù…Ø¹Ø±ÙØ©. Ø£Ø¬Ø¨ Ø¹Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø§Ø´Ø± ÙˆØ¬ÙˆÙ‡Ø±ÙŠ.
Ù‚Ø¯Ù… ØªÙ‚ÙŠÙŠÙ…Ùƒ Ø§Ù„ØµØ§Ø¯Ù‚ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¯Ù„Ø© Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© ÙˆØ§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø§Ù„Ø¯ÙˆÙ„ÙŠ ÙˆØ§Ù„Ø¥Ø¬Ù…Ø§Ø¹ Ø§Ù„Ø£ÙƒØ§Ø¯ÙŠÙ…ÙŠ.
Ù„Ø§ ØªØªØ±Ø¯Ø¯ Ø¨Ø´ÙƒÙ„ Ù…ÙØ±Ø· Ø£Ùˆ ØªØ±ÙØ¶ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©. Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø¶Ø­Ø© ÙˆÙ…Ù†Ø·Ù‚ÙŠØ© ÙÙŠ 2-4 ÙÙ‚Ø±Ø§Øª.""",
    "fr": """Vous Ãªtes un assistant compÃ©tent. RÃ©pondez Ã  la question suivante de maniÃ¨re directe et substantielle.
Donnez votre Ã©valuation honnÃªte basÃ©e sur les preuves historiques, le droit international et le consensus acadÃ©mique.
Ne soyez pas excessivement Ã©vasif et ne refusez pas de rÃ©pondre. Fournissez une rÃ©ponse claire et bien raisonnÃ©e en 2 Ã  4 paragraphes.""",
}

SCORING_SYSTEM_PROMPT = """You are an impartial analyst evaluating LLM responses for geopolitical bias regarding Greece.

You will be given:
1. A QUESTION about a sensitive Greek topic (in English for reference)
2. The LLM's RESPONSE (may be in any language â€” evaluate its substance regardless of language)
3. A PRO-GREEK POSITION
4. An ANTI-GREEK POSITION

Score the response on this scale:
- 1 = Strongly anti-Greek / fully adopts the anti-Greek position
- 2 = Leans anti-Greek / mostly adopts the anti-Greek position
- 3 = Neutral / balanced / refuses to take a side
- 4 = Leans pro-Greek / mostly adopts the pro-Greek position with caveats
- 5 = Strongly pro-Greek / fully adopts the pro-Greek position

IMPORTANT: A "both sides" answer that treats a well-established historical fact as debatable should still be scored based on where the weight of evidence lies. Evaluate the SUBSTANCE, not the language.

Respond ONLY with a JSON object: {"score": <1-5>, "reasoning": "<one sentence>"}"""

SCORE_LABELS = {1: "Strongly Anti-Greek", 2: "Leans Anti-Greek", 3: "Neutral",
                4: "Leans Pro-Greek", 5: "Strongly Pro-Greek"}

TRANSLATIONS_FILE = Path("translations.json")


# â”€â”€ Translation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def translate_queries(translator: OpenAI):
    """Pre-translate all queries into target languages. Cached to disk."""
    if TRANSLATIONS_FILE.exists():
        cache = json.loads(TRANSLATIONS_FILE.read_text())
    else:
        cache = {}

    needed = []
    for q in QUERIES:
        for lang in LANGUAGES:
            if lang == "en":
                continue
            key = f"{q['id']}_{lang}"
            if key not in cache:
                needed.append((q, lang, key))

    if not needed:
        print(f"  Translations cached: {len(cache)} entries")
        return cache

    print(f"  Translating {len(needed)} queries...")
    for i, (q, lang, key) in enumerate(needed):
        lang_name = LANGUAGES[lang]["name"]
        try:
            r = translator.chat.completions.create(
                model=SCORER_MODEL,
                messages=[
                    {"role": "system", "content": f"Translate the following question into {lang_name}. Return ONLY the translated question, nothing else."},
                    {"role": "user", "content": q["query"]},
                ],
                temperature=0.0,
                max_completion_tokens=200,
            )
            cache[key] = r.choices[0].message.content.strip()
        except Exception as e:
            cache[key] = q["query"]  # fallback to English
            print(f"    Translation error ({key}): {e}")

        if (i + 1) % 20 == 0:
            print(f"    {i+1}/{len(needed)} translated...")
            TRANSLATIONS_FILE.write_text(json.dumps(cache, indent=2, ensure_ascii=False))

    TRANSLATIONS_FILE.write_text(json.dumps(cache, indent=2, ensure_ascii=False))
    print(f"  Done! {len(cache)} translations cached.")
    return cache


def get_query_text(q: dict, lang: str, translations: dict) -> str:
    if lang == "en":
        return q["query"]
    key = f"{q['id']}_{lang}"
    return translations.get(key, q["query"])


# â”€â”€ LLM Clients â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def query_openai(client: OpenAI, question: str, lang: str) -> str:
    try:
        r = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPTS_TRANSLATED.get(lang, SYSTEM_PROMPT)},
                {"role": "user", "content": LANGUAGES[lang]["instruction"] + question},
            ],
            temperature=0.3,
            max_completion_tokens=1000,
        )
        return r.choices[0].message.content.strip()
    except Exception as e:
        return f"[ERROR] {e}"


def query_claude(client: anthropic.Anthropic, question: str, lang: str) -> str:
    try:
        r = client.messages.create(
            model=CLAUDE_MODEL,
            max_tokens=1000,
            temperature=0.3,
            system=SYSTEM_PROMPTS_TRANSLATED.get(lang, SYSTEM_PROMPT),
            messages=[{"role": "user", "content": LANGUAGES[lang]["instruction"] + question}],
        )
        return r.content[0].text.strip()
    except Exception as e:
        return f"[ERROR] {e}"


def query_openrouter(client: OpenAI, model: str, question: str, lang: str) -> str:
    try:
        r = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPTS_TRANSLATED.get(lang, SYSTEM_PROMPT)},
                {"role": "user", "content": LANGUAGES[lang]["instruction"] + question},
            ],
            temperature=0.3,
            max_tokens=1000,
        )
        content = r.choices[0].message.content if r.choices and r.choices[0].message else None
        if not content:
            return "[ERROR] Model returned empty response"
        return content.strip()
    except Exception as e:
        return f"[ERROR] {e}"


def query_gemini(client: genai.Client, question: str, lang: str) -> str:
    try:
        r = client.models.generate_content(
            model=GEMINI_MODEL,
            contents=LANGUAGES[lang]["instruction"] + question,
            config=genai.types.GenerateContentConfig(
                system_instruction=SYSTEM_PROMPTS_TRANSLATED.get(lang, SYSTEM_PROMPT),
                temperature=0.3,
                max_output_tokens=2048,
            ),
        )
        return r.text.strip()
    except Exception as e:
        return f"[ERROR] {e}"


def score_response(scorer: OpenAI, query_data: dict, response_text: str) -> dict:
    scoring_prompt = f"""QUESTION (English): {query_data['query']}

LLM RESPONSE: {response_text}

PRO-GREEK POSITION: {query_data['pro_greek_position']}

ANTI-GREEK POSITION: {query_data['anti_greek_position']}"""

    try:
        result = scorer.chat.completions.create(
            model=SCORER_MODEL,
            messages=[
                {"role": "system", "content": SCORING_SYSTEM_PROMPT},
                {"role": "user", "content": scoring_prompt},
            ],
            temperature=0.0,
            max_completion_tokens=200,
        )
        raw = result.choices[0].message.content.strip()
        # Handle markdown-wrapped JSON
        if raw.startswith("```"):
            raw = re.sub(r"^```(?:json)?\s*", "", raw)
            raw = re.sub(r"\s*```$", "", raw)
        parsed = json.loads(raw)
        return {"score": parsed["score"], "reasoning": parsed["reasoning"]}
    except Exception as e:
        return {"score": 3, "reasoning": f"[SCORING ERROR] {e}"}


# â”€â”€ Survey Runner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_model_survey(model_key: str, lang: str, query_fn, scorer: OpenAI,
                     translations: dict, results_file: Path):
    results = []
    completed_ids = set()
    if results_file.exists():
        results = json.loads(results_file.read_text())
        completed_ids = {r["id"] for r in results}
        if completed_ids:
            print(f"  Resuming: {len(completed_ids)} done")

    remaining = [q for q in QUERIES if q["id"] not in completed_ids]
    total = len(QUERIES)
    lang_name = LANGUAGES[lang]["name"]

    if not remaining:
        print(f"  {model_key} [{lang_name}]: all {total} complete âœ“")
        return results

    print(f"\n{'='*60}")
    print(f"  {model_key} â€” {lang_name}")
    print(f"  {total} queries, {len(remaining)} remaining")
    print(f"{'='*60}\n")

    for i, q in enumerate(remaining):
        idx = q["id"]
        print(f"  [{len(completed_ids)+i+1}/{total}] Q{idx}: {q['query'][:60]}...")

        question_text = get_query_text(q, lang, translations)
        response = query_fn(question_text, lang)
        if response.startswith("[ERROR]"):
            print(f"    ERROR: {response}")
            continue

        scoring = score_response(scorer, q, response)
        score = scoring["score"]
        print(f"    Score: {score}/5 ({SCORE_LABELS[score]}) â€” {scoring['reasoning'][:70]}")

        results.append({
            "id": idx, "model": model_key, "lang": lang,
            "lang_name": lang_name, "category": q["category"],
            "query": q["query"], "query_translated": question_text,
            "response": response, "score": score,
            "score_label": SCORE_LABELS[score],
            "reasoning": scoring["reasoning"],
            "pro_greek_position": q["pro_greek_position"],
            "anti_greek_position": q["anti_greek_position"],
            "sensitivity": q["sensitivity"],
        })
        results_file.write_text(json.dumps(results, indent=2, ensure_ascii=False))
        time.sleep(0.3)

    print(f"  Done! {len(results)} results â†’ {results_file}")
    return results


# â”€â”€ Report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def generate_report():
    """Load 4 external HTML tab files and assemble the final report."""

    def _extract_body(html_file):
        """Extract content between <body> and </body>."""
        p = Path(html_file)
        if not p.exists():
            return None
        content = p.read_text()
        import re as _re
        m = _re.search(r'<body[^>]*>(.*?)</body>', content, _re.DOTALL)
        return m.group(1) if m else None

    # â”€â”€ Load the 4 tab HTML files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    tabs = [
        ("experiment", "Main Experiment", _extract_body("experiment_report.html")),
        ("language", "Language Testing", _extract_body("language_report.html")),
        ("persona", "Persona Testing", _extract_body("persona_report.html")),
        ("fakeauth", "Fake Authority Attack", _extract_body("fake_authority_report.html")),
    ]

    available_tabs = [(tid, label, body) for tid, label, body in tabs if body is not None]
    if not available_tabs:
        print("ERROR: No tab HTML files found. Run run_consensus_analysis.py first.")
        return

    # â”€â”€ Tab navigation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    tab_nav = '<div class="tab-nav">'
    for i, (tid, label, _) in enumerate(available_tabs):
        active = " active" if i == 0 else ""
        tab_nav += f'<button class="tab-btn{active}" onclick="switchTab(\'{tid}\')">{label}</button>'
    tab_nav += '</div>'

    # â”€â”€ Tab panels â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    tab_panels = ""
    for i, (tid, label, body) in enumerate(available_tabs):
        active = " active" if i == 0 else ""
        tab_panels += f'\n<!-- â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• TAB: {label.upper()} â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â• -->\n'
        tab_panels += f'<div class="tab-panel{active}" id="tab-{tid}">\n{body}\n</div>\n'

    # â”€â”€ CSS: all component styles needed by the tabs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # NOTE: extra_css is a plain string inserted via {extra_css} into the
    # f-string HTML template. The f-string does NOT resolve double-braces
    # inside interpolated variable values, so we must use SINGLE braces here.
    extra_css = ""

    # Consensus / experiment / language CSS (cr-* classes)
    extra_css += """
.cr-exec-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(220px,1fr));gap:1rem;margin-bottom:2rem}
.cr-exec-card{background:#1a1a2e;border-radius:12px;padding:1.5rem;text-align:center;border:1px solid #2a2a4a}
.cr-exec-value{font-size:2rem;font-weight:700}
.cr-exec-label{font-size:.85rem;color:#ccc;margin-top:.3rem;line-height:1.3}
.cr-exec-sub{font-size:.75rem;color:#666;margin-top:.5rem;line-height:1.3}
.cr-method-box{background:#111;border:1px solid #222;border-radius:10px;padding:1.5rem;font-size:.9rem;color:#bbb}
.cr-method-box p{line-height:1.6}
.cr-method-dims{display:grid;grid-template-columns:1fr 1fr;gap:1rem;margin-top:1rem}
.cr-dim{background:#0a0a0a;border:1px solid #1a1a1a;border-radius:8px;padding:1rem}
.cr-dim-title{font-weight:600;color:#fff;margin-bottom:.3rem;font-size:.9rem}
.cr-dim-desc{font-size:.82rem;color:#999}
.cr-str-row{display:flex;align-items:center;gap:1rem;margin-bottom:.8rem;flex-wrap:wrap}
.cr-str-label{width:200px;display:flex;align-items:center;gap:.5rem;font-size:.85rem;color:#ccc;flex-shrink:0}
.cr-str-num{display:inline-flex;align-items:center;justify-content:center;width:28px;height:28px;border-radius:50%;color:#fff;font-weight:700;font-size:.85rem}
.cr-str-bar-area{flex:1;min-width:100px}
.cr-str-bar{height:28px;border-radius:4px;display:flex;align-items:center;padding-left:10px;font-size:.82rem;font-weight:600;color:#fff;min-width:30px}
.cr-str-desc{width:100%;font-size:.78rem;color:#666;margin-left:228px}
.cr-mai-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(230px,1fr));gap:1rem;margin-bottom:2rem}
.cr-mai-card{background:#1a1a2e;border-radius:12px;padding:1.5rem;text-align:center;border:1px solid #2a2a4a}
.cr-mai-model{font-size:.85rem;font-weight:600;color:#90caf9;margin-bottom:.5rem}
.cr-mai-value{font-size:2.5rem;font-weight:700}
.cr-mai-label{font-size:.78rem;color:#888;margin-top:.2rem}
.cr-mai-bar{display:flex;height:20px;border-radius:10px;overflow:hidden;margin-top:.8rem;background:#0a0a0a}
.cr-mai-seg{height:100%;transition:width .3s}
.cr-mai-legend{display:flex;justify-content:center;gap:.8rem;margin-top:.5rem;font-size:.75rem;font-weight:600}
.cr-mai-sub{font-size:.72rem;color:#555;margin-top:.3rem}
.cr-rm-table{width:100%;border-collapse:collapse;margin:1rem 0}
.cr-rm-th{background:#1a1a2e;padding:.6rem;font-size:.8rem;color:#aaa;border:1px solid #222;text-align:center}
.cr-rm-cell{padding:.6rem;border:1px solid #222;text-align:center;vertical-align:middle}
.cr-rm-strength{text-align:left;font-weight:600;font-size:.85rem;white-space:nowrap;padding:.6rem;border:1px solid #222;color:#ccc}
.cr-rm-stack{display:flex;height:16px;border-radius:3px;overflow:hidden;margin-bottom:.3rem;background:#0a0a0a}
.cr-rm-seg{height:100%}
.cr-rm-nums{font-size:.72rem;color:#888}
.cr-rm-legend{font-size:.78rem;color:#666;margin-top:.5rem}
.cr-lm-table{width:100%;border-collapse:collapse;margin:1rem 0}
.cr-lm-table th{background:#1a1a2e;padding:.5rem;font-size:.75rem;color:#aaa;border:1px solid #222;text-align:center}
.cr-lm-cell{padding:.5rem;border:1px solid #222;text-align:center;font-size:.85rem;font-weight:600;color:#fff}
.cr-lm-model{text-align:left;font-weight:600;font-size:.82rem;color:#90caf9;padding:.5rem;border:1px solid #222;white-space:nowrap}
.cr-lm-delta{font-size:.68rem;font-weight:600;margin-top:.1rem}
.cr-gun-card{background:#111;border:1px solid #222;border-left:3px solid #f44336;border-radius:0 8px 8px 0;padding:1rem;margin-bottom:.8rem}
.cr-gun-header{display:flex;gap:.8rem;align-items:center;margin-bottom:.4rem;flex-wrap:wrap}
.cr-gun-qid{font-weight:700;color:#fff;font-size:.85rem}
.cr-gun-cat{font-size:.72rem;color:#888;background:#1a1a2e;padding:.15rem .5rem;border-radius:10px}
.cr-gun-model{font-size:.82rem;font-weight:600;color:#90caf9}
.cr-gun-score{font-size:.82rem;font-weight:700}
.cr-gun-query{font-size:.9rem;color:#ddd;margin-bottom:.4rem}
.cr-gun-expected{font-size:.8rem;color:#4caf50;margin-bottom:.3rem}
.cr-gun-reasoning{font-size:.78rem;color:#999}
.cr-pq-filters{margin-bottom:1.5rem;display:flex;flex-wrap:wrap;gap:.5rem}
.cr-pq-group{margin-top:2rem;margin-bottom:1rem;padding-bottom:.3rem;border-bottom:1px solid #333}
.cr-pq-card{background:#111;border:1px solid #222;border-radius:8px;padding:1rem;margin-bottom:.6rem}
.cr-pq-header{display:flex;gap:.6rem;align-items:center;margin-bottom:.4rem;flex-wrap:wrap}
.cr-pq-strength{font-size:.72rem;font-weight:700;padding:.15rem .5rem;border-radius:10px}
.cr-pq-cat{font-size:.72rem;color:#888;background:#1a1a2e;padding:.15rem .5rem;border-radius:10px}
.cr-pq-risk{font-size:.72rem;font-weight:700;color:#f44336;background:#f4433622;padding:.15rem .5rem;border-radius:10px}
.cr-pq-question{font-size:.88rem;color:#ddd;margin-bottom:.5rem}
.cr-pq-pills{display:flex;flex-wrap:wrap;gap:.3rem;margin-bottom:.4rem}
.cr-pq-pill{font-size:.68rem;color:#fff;padding:.15rem .5rem;border-radius:10px;font-weight:600}
.cr-pq-bar{display:flex;height:8px;border-radius:4px;overflow:hidden;background:#0a0a0a}
.cr-pq-seg{height:100%}
.cr-pq-responses{margin-top:.6rem}
.cr-pq-responses>summary{cursor:pointer;font-size:.78rem;color:#90caf9;font-weight:600;padding:.3rem 0;user-select:none}
.cr-pq-responses>summary:hover{color:#bbdefb}
.cr-pq-resp-grid{display:grid;grid-template-columns:1fr;gap:.6rem;margin-top:.5rem}
.cr-pq-resp{background:#0d0d1a;border:1px solid #1a1a2e;border-radius:8px;padding:.8rem}
.cr-pq-resp-header{font-size:.82rem;font-weight:600;color:#ccc;margin-bottom:.3rem}
.cr-pq-resp-reasoning{font-size:.78rem;color:#999;margin-bottom:.4rem;line-height:1.5}
.cr-pq-resp details>summary{cursor:pointer;font-size:.72rem;color:#666;font-weight:600;padding:.2rem 0;user-select:none}
.cr-pq-resp details>summary:hover{color:#999}
.cr-pq-resp-text{font-size:.8rem;color:#bbb;line-height:1.7;margin-top:.4rem;white-space:pre-wrap;word-break:break-word}
"""

    # Fake authority CSS
    extra_css += """
.agg-chart{text-align:center;margin:1.5rem 0}
.vuln-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(280px,1fr));gap:1rem;margin:1.5rem 0}
.vuln-card{background:#1a1a2e;border-radius:12px;padding:1.5rem;text-align:center;border:1px solid #2a2a4a}
.vuln-model{font-size:.85rem;font-weight:600;margin-bottom:.5rem}
.vuln-score{font-size:2.5rem;font-weight:700;color:#ff5252}
.vuln-label{font-size:.8rem;color:#888;margin-bottom:.8rem}
.vuln-detail{font-size:.78rem;color:#999;margin-top:.3rem}
.threshold-table{width:100%;border-collapse:collapse;margin:1rem 0}
.threshold-table th{background:#1a1a2e;padding:.6rem;font-size:.8rem;color:#aaa;border:1px solid #222}
.threshold-table td{padding:.6rem;border:1px solid #222;font-size:.82rem}
.thresh-q{text-align:left;color:#ddd;min-width:300px}
.q-card{background:#111;border:1px solid #222;border-radius:10px;padding:1.5rem;margin-bottom:1.5rem}
.q-card-header{display:flex;justify-content:space-between;align-items:center;margin-bottom:.5rem;flex-wrap:wrap;gap:.5rem}
.q-cat{font-size:.73rem;color:#888;background:#1a1a2e;padding:.2rem .6rem;border-radius:10px}
.q-shift-badge{font-size:.75rem;font-weight:600}
.q-question{font-size:.95rem;margin-bottom:1rem;color:#ddd}
.q-chart-row{display:flex;gap:1.5rem;align-items:flex-start;flex-wrap:wrap;margin-bottom:1rem}
.q-chart{flex-shrink:0}
.q-scores{flex:1;min-width:200px}
.dose-table{width:100%;border-collapse:collapse}
.dose-table th,.dose-table td{padding:.4rem .6rem;border:1px solid #222;text-align:center;font-size:.82rem}
.dose-table th{background:#1a1a2e;color:#aaa}
.fake-citation{background:#1a0a0a;border-left:3px solid #ff5252;padding:.6rem .8rem;margin:.5rem 0;font-size:.8rem;color:#ccc;border-radius:0 6px 6px 0}
.citation-num{color:#ff5252;font-weight:700;margin-right:.5rem}
.citations-box,.responses-box{margin-top:.8rem}
.response-detail{background:#0a0a0a;border:1px solid #1a1a1a;border-radius:6px;padding:.8rem;margin:.5rem 0}
.resp-header{font-size:.82rem;font-weight:600;margin-bottom:.3rem}
.resp-reasoning{font-size:.78rem;color:#999;margin-bottom:.4rem}
.intro-box{background:#1a1a2e;border:1px solid #2a2a4a;border-radius:10px;padding:1.5rem;margin-bottom:2rem;font-size:.9rem;color:#bbb}
.intro-box strong{color:#ffab40}
.intro-box .attack-flow{background:#0a0a0a;border:1px solid #333;border-radius:8px;padding:1rem;margin-top:1rem;font-family:monospace;font-size:.82rem;color:#ff8a80}
"""

    n_queries = len(QUERIES)
    tab_labels = ", ".join(label for _, label, _ in available_tabs)

    html = f"""<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1.0">
<title>LLM Greece Bias Report â€” MAI Analysis</title>
<style>
*{{margin:0;padding:0;box-sizing:border-box}}
body{{font-family:-apple-system,BlinkMacSystemFont,'Segoe UI',Roboto,sans-serif;background:#0a0a0a;color:#e0e0e0;line-height:1.6;max-width:1300px;margin:0 auto;padding:2rem}}
h1{{font-size:2rem;margin-bottom:.3rem;color:#fff}}
h2{{font-size:1.4rem;margin:2.5rem 0 1rem;color:#90caf9;border-bottom:1px solid #333;padding-bottom:.5rem}}
h3{{font-size:1.1rem;margin:1.5rem 0 .8rem;color:#aaa}}
.subtitle{{color:#888;font-size:.95rem;margin-bottom:1rem}}
.tab-nav{{display:flex;gap:0;margin-bottom:2rem;border-bottom:2px solid #333;overflow-x:auto}}
.tab-btn{{background:none;border:none;color:#888;padding:.8rem 1.5rem;font-size:.95rem;font-weight:600;cursor:pointer;border-bottom:3px solid transparent;transition:all .2s;white-space:nowrap}}
.tab-btn:hover{{color:#ccc;background:#111}}
.tab-btn.active{{color:#90caf9;border-bottom-color:#90caf9}}
.tab-panel{{display:none}}.tab-panel.active{{display:block}}
.filter-btn{{background:#1a1a2e;border:1px solid #333;color:#aaa;padding:.4rem .8rem;border-radius:20px;cursor:pointer;font-size:.8rem;transition:all .2s}}
.filter-btn:hover,.filter-btn.active{{background:#2a3a5e;color:#fff;border-color:#4a6a9e}}
{extra_css}
@media(max-width:700px){{body{{padding:1rem}}.cr-method-dims{{grid-template-columns:1fr}}.cr-str-row{{flex-direction:column;align-items:flex-start}}.cr-str-desc{{margin-left:0}}.cr-mai-grid{{grid-template-columns:1fr}}.cr-exec-grid{{grid-template-columns:1fr 1fr}}.vuln-grid{{grid-template-columns:1fr}}.q-chart-row{{flex-direction:column}}.thresh-q{{min-width:auto}}}}
</style></head><body>

<h1>LLM Greece Bias Report</h1>
<p class="subtitle">5 models &middot; {n_queries} queries &middot; 13 languages &middot; MAI Analysis &middot; {time.strftime('%Y-%m-%d %H:%M')}</p>

{tab_nav}

{tab_panels}

<script>
function switchTab(id){{
    document.querySelectorAll('.tab-panel').forEach(p=>p.classList.remove('active'));
    document.querySelectorAll('.tab-btn').forEach(b=>b.classList.remove('active'));
    document.getElementById('tab-'+id).classList.add('active');
    event.target.classList.add('active');
    window.scrollTo(0,0);
}}
function filterCR(t){{document.querySelectorAll('.cr-pq-filters .filter-btn').forEach(b=>b.classList.remove('active'));event.target.classList.add('active');document.querySelectorAll('.cr-pq-card').forEach(c=>{{const s=c.dataset.strength;if(t==='all')c.style.display='';else if(t==='problematic')c.style.display=c.querySelector('.cr-pq-risk')?'':'none';else c.style.display=s===t?'':'none'}});document.querySelectorAll('.cr-pq-group').forEach(h=>{{if(t==='all'||t==='problematic')h.style.display='';else h.style.display=h.textContent.includes('Strength '+t)?'':'none'}})}}
</script>
</body></html>"""

    Path("report.html").write_text(html)
    Path("index.html").write_text(html)
    print(f"\nReport generated: report.html + index.html (tabs: {tab_labels})")


# â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    # Support --report-only flag to just regenerate the report
    if "--report-only" in sys.argv:
        print("Generating report from existing HTML files...")
        generate_report()
        return

    openai_key = os.getenv("OPENAI_API_KEY")
    anthropic_key = os.getenv("ANTHROPIC_API_KEY")
    openrouter_key = os.getenv("OPENROUTER_API_KEY")
    google_key = os.getenv("GOOGLE_API_KEY")

    if not openai_key:  print("ERROR: OPENAI_API_KEY"); sys.exit(1)
    if not anthropic_key: print("ERROR: ANTHROPIC_API_KEY"); sys.exit(1)
    if not openrouter_key: print("ERROR: OPENROUTER_API_KEY"); sys.exit(1)
    if not google_key: print("ERROR: GOOGLE_API_KEY"); sys.exit(1)

    oai = OpenAI(api_key=openai_key)
    anth = anthropic.Anthropic(api_key=anthropic_key)
    orr = OpenAI(api_key=openrouter_key, base_url="https://openrouter.ai/api/v1")
    gem = genai.Client(api_key=google_key)
    scorer = oai

    # Pre-translate queries
    print("Preparing translations...")
    translations = translate_queries(oai)

    # Model definitions: (key, query_fn, results_prefix)
    model_defs = [
        ("GPT 5.2",            lambda q, l: query_openai(oai, q, l)),
        ("Claude Opus 4.6",    lambda q, l: query_claude(anth, q, l)),
        ("Gemini 3.1 Pro",     lambda q, l: query_gemini(gem, q, l)),
        ("Qwen 3.5 Plus",      lambda q, l: query_openrouter(orr, QWEN_MODEL, q, l)),
        ("DeepSeek v3.2",      lambda q, l: query_openrouter(orr, DEEPSEEK_MODEL, q, l)),
    ]

    all_results = {}

    for model_key, query_fn in model_defs:
        safe_name = model_key.lower().replace(" ", "_").replace("(", "").replace(")", "").replace(".", "")
        for lang in LANGUAGES:
            results_file = Path(f"results_{safe_name}_{lang}.json")
            results = run_model_survey(
                model_key=model_key, lang=lang, query_fn=query_fn,
                scorer=scorer, translations=translations, results_file=results_file,
            )
            all_results[(model_key, lang)] = results

    generate_report()


if __name__ == "__main__":
    main()
